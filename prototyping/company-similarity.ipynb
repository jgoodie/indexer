{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a555c32-7dfd-448e-8a75-6ef7bf19f4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from itertools import combinations, permutations\n",
    "\n",
    "import spacy\n",
    "from thinc.api import set_gpu_allocator, require_gpu\n",
    "import pyate\n",
    "\n",
    "import tika\n",
    "from tika import parser\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer, TreebankWordTokenizer\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "# pytorch library\n",
    "import gc\n",
    "import GPUtil\n",
    "import torch # the main pytorch library\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as f # the sub-library containing different functions for manipulating with tensors\n",
    "\n",
    "# huggingface's transformers library\n",
    "from transformers import BertModel, BertTokenizer, AutoTokenizer, AutoModel\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cfc54927-50f8-4a6c-a5a8-d9ec04acd329",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install GPUtil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eaedd1e7-0cea-482b-b6ad-a646101bcd98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% |  0% |\n",
      "|  1 |  0% |  0% |\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "set_gpu_allocator(\"pytorch\")\n",
    "print(require_gpu(0))\n",
    "# torch.cuda.is_available()\n",
    "# torch.cuda.current_device()\n",
    "# torch.cuda.device_count()\n",
    "#torch.cuda.get_device_name(0)\n",
    "print(GPUtil.showUtilization())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85a3a07-3c4b-4d5f-89f9-059a06c46a33",
   "metadata": {},
   "source": [
    "### Extract Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e46172c9-7da9-4c30-9f92-44c5c8177489",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(pdf, remove_stop_words=False):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    raw = parser.from_file(pdf)\n",
    "    content = raw['content'].strip()\n",
    "    datepat = r\"[0-9]{0,2}/[0-9]{0,2}/[0-9]{0,2}, [0-9]{0,2}:[0-9]{0,2} [AP]M\"\n",
    "    sents = [s.strip() for s in sent_tokenize(content)]\n",
    "    sents = [re.sub(datepat, '', s) for s in sents]\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    pattern = r\"Page [0-9]{0,3} of [0-9]{0,3}\"\n",
    "    urlpat = r'^https?:\\/\\/.*[\\r\\n]*'\n",
    "    sents = [re.sub(pattern, '', s) for s in sents]\n",
    "    sents = [re.sub(urlpat, '', s, flags=re.MULTILINE) for s in sents]\n",
    "    if remove_stop_words:\n",
    "        sent_tokens = [tokenizer.tokenize(s) for s in sents]\n",
    "        stripped_sents = []\n",
    "        for sent in sent_tokens:\n",
    "            temp_sent = []\n",
    "            for t in sent:\n",
    "                if t.lower() not in stop_words:\n",
    "                    temp_sent.append(t)\n",
    "            stripped_sents.append(\" \".join(temp_sent))\n",
    "        return \" \".join(stripped_sents)\n",
    "    else:\n",
    "        sents = [\" \".join(tokenizer.tokenize(s)) for s in sents] # why??\n",
    "        return \" \".join(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0925fc0d-0c58-4a4f-9591-1eecb5e783dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sents(pdf):\n",
    "    raw = parser.from_file(pdf)\n",
    "    content = raw['content'].strip()\n",
    "    datepat = r\"[0-9]{0,2}/[0-9]{0,2}/[0-9]{0,2}, [0-9]{0,2}:[0-9]{0,2} [AP]M\"\n",
    "    sents = [s.strip() for s in sent_tokenize(content)]\n",
    "    sents = [re.sub(datepat, '', s) for s in sents]\n",
    "    pattern = r\"Page [0-9]{0,3} of [0-9]{0,3}\"\n",
    "    urlpat = r'^https?:\\/\\/.*[\\r\\n]*'\n",
    "    sents = [re.sub(pattern, '', s) for s in sents]\n",
    "    sents = [re.sub(urlpat, '', s, flags=re.MULTILINE) for s in sents]\n",
    "    sents = [re.sub(r'\\n', ' ', s) for s in sents]\n",
    "    sents = [s for s in sents if s != '']\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    sents = [\" \".join(tokenizer.tokenize(s.lower())) for s in sents]\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "41c29c4f-6be0-4058-9eab-ef3739b789e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim(doc, text, n=1):\n",
    "    \"\"\"\n",
    "    Take a single doc from a corpus and a text string and find the similar sentences\n",
    "    \"\"\"\n",
    "    t = []\n",
    "    text = text.strip().lower()\n",
    "    content = [s.strip().lower() for s in doc ]\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(content)\n",
    "    Xquery = vectorizer.transform([text])\n",
    "    sims = cosine_similarity(X, Xquery).flatten()\n",
    "    top_n = np.argsort(sims)[::-1][:n]\n",
    "    for i in top_n:\n",
    "        t.append((doc[i], sims[i]))\n",
    "    return t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa2985f-d0ea-45a4-b46d-58db40996532",
   "metadata": {},
   "source": [
    "## OpenVault"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a5015126-f136-45e2-913a-edbb4608b0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../mr_minio/data/openvault\"\n",
    "pdfs = glob(path+'/*Jersey*Open*pdf')\n",
    "\n",
    "openvault = \" \".join([extract(pdf, remove_stop_words=True) for pdf in pdfs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b49ac3-af65-4a82-9807-846cc8d980f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6431d218-3466-47fc-91a6-a77bfe97b738",
   "metadata": {},
   "source": [
    "## Federos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "29156c7a-207d-49a2-b898-833bb32392f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../mr_minio/data/openvault\"\n",
    "pdfs = glob(path+'/*Feder*pdf')\n",
    "\n",
    "federos = \" \".join([extract(pdf, remove_stop_words=True) for pdf in pdfs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2c7f8287-3592-450a-80d5-24ce9ca751f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(federos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e474136-64cb-4bcf-9488-f9e5a87f58d5",
   "metadata": {},
   "source": [
    "## Incognito"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ad49fdec-387b-4762-a32e-8da58682103c",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../mr_minio/data/openvault\"\n",
    "pdfs = glob(path+'/*Vancouver*pdf')\n",
    "\n",
    "incognito = \" \".join([extract(pdf, remove_stop_words=True) for pdf in pdfs])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b12cff-b65e-41cd-88cc-04d45fdc8e0c",
   "metadata": {},
   "source": [
    "## Intraway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bacd78a1-c26a-4792-8644-cc8985ae598c",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../mr_minio/data/openvault\"\n",
    "pdfs = glob(path+'/*Buenos*pdf')\n",
    "\n",
    "intraway = \" \".join([extract(pdf, remove_stop_words=True) for pdf in pdfs])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21964d8-840b-442a-8792-964f6286d1c3",
   "metadata": {},
   "source": [
    "## Company Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "15e41301-3a40-4fca-8500-189e8dddf89c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.3316285 , 0.44533993, 0.35677711])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "corpus = [federos, incognito, intraway]\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=stop_words)\n",
    "#vectorizer = TfidfVectorizer()\n",
    "\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "Xquery = vectorizer.transform([openvault])\n",
    "\n",
    "sims = cosine_similarity(X, Xquery).flatten()\n",
    "\n",
    "sims"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc93ab3c-934d-4f1c-9c94-a7c8c7cde82d",
   "metadata": {},
   "source": [
    "## Per Document Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6445f300-74c8-4381-9c1d-ec9c8cb7c8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../mr_minio/data/openvault\"\n",
    "ovpdfs= glob(path+'/*Jersey*Open*pdf')\n",
    "\n",
    "path = \"../mr_minio/data/openvault\"\n",
    "fedpdfs = glob(path+'/*Feder*pdf')\n",
    "\n",
    "path = \"../mr_minio/data/openvault\"\n",
    "incogpdfs = glob(path+'/*Vancouver*pdf')\n",
    "\n",
    "path = \"../mr_minio/data/openvault\"\n",
    "intrapdfs = glob(path+'/*Buenos*pdf')\n",
    "\n",
    "ovdocs = [extract(doc, remove_stop_words=True) for doc in ovpdfs]\n",
    "feddocs = [extract(doc, remove_stop_words=True) for doc in fedpdfs]\n",
    "incogdocs = [extract(doc, remove_stop_words=True) for doc in incogpdfs]\n",
    "intradocs = [extract(doc, remove_stop_words=True) for doc in intrapdfs]\n",
    "\n",
    "ovdict = defaultdict()\n",
    "for i, d in enumerate(ovpdfs):\n",
    "    filename = ovpdfs[i].split('/')[-1]\n",
    "    ovdict[filename] = ovdocs[i]\n",
    "    \n",
    "feddict = defaultdict()\n",
    "for i, d in enumerate(fedpdfs):\n",
    "    filename = fedpdfs[i].split('/')[-1]\n",
    "    feddict[filename] = feddocs[i]\n",
    "    \n",
    "incogdict = defaultdict()\n",
    "for i, d in enumerate(incogpdfs):\n",
    "    filename = incogpdfs[i].split('/')[-1]\n",
    "    incogdict[filename] = incogdocs[i]\n",
    "    \n",
    "intradict = defaultdict()\n",
    "for i, d in enumerate(intrapdfs):\n",
    "    filename = intrapdfs[i].split('/')[-1]\n",
    "    intradict[filename] = intradocs[i]\n",
    "    \n",
    "doctuples = []\n",
    "for ov in ovdict.items():\n",
    "    for fed in feddict.items():\n",
    "        doctuples.append((ov[0], fed[0]))\n",
    "        \n",
    "for ov in ovdict.items():\n",
    "    for incog in incogdict.items():\n",
    "        doctuples.append((ov[0], incog[0]))\n",
    "        \n",
    "for ov in ovdict.items():\n",
    "    for intra in intradict.items():\n",
    "        doctuples.append((ov[0], intra[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b599c0-8600-4f6d-87b4-59a903611506",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "simscores = []\n",
    "bp = \"../mr_minio/data/openvault/\"\n",
    "for i in range(len(doctuples)):\n",
    "    vectorizer = TfidfVectorizer(stop_words=stop_words)\n",
    "    X = vectorizer.fit_transform([extract(bp+doctuples[i][1], remove_stop_words=True).lower()])\n",
    "    Xquery = vectorizer.transform([extract(bp+doctuples[i][0], remove_stop_words=True).lower()])\n",
    "    sims = cosine_similarity(X, Xquery).flatten()\n",
    "    simscores.append((doctuples[i], sims[0]))\n",
    "simscores = dict(simscores)\n",
    "simscores = {k: v for k, v in sorted(simscores.items(), key=lambda item: item[1], reverse=True)}\n",
    "sc = list(simscores.items())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a66e96-e9ab-420e-a8e4-0f269c6be5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfef58cb-c209-4cf6-a2a8-777f539a949e",
   "metadata": {},
   "source": [
    "## PyATE\n",
    "\n",
    "https://spacy.io/universe/project/pyate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf5456b-5414-4cae-ada3-647692885230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bp = \"../mr_minio/data/openvault/\"\n",
    "# ovdoc1 = extract(bp+sc[0][0][0], remove_stop_words=True)\n",
    "# incogdoc2 = extract(bp+sc[0][0][1], remove_stop_words=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b58edbf-c9cb-4e8d-a6a7-2e70fe8ff75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp = spacy.load('en_core_web_trf')\n",
    "# # nlp = spacy.load('en_core_web_lg')\n",
    "# nlp.add_pipe(\"combo_basic\") # or any of `basic`, `weirdness`, `term_extractor` or `cvalue`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694cde6f-0171-41f5-8a69-52e9761d1977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # del nlp\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14db5b6c-c008-4f1c-a4cb-a6ba44076c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc_ov = nlp(ovdoc1)\n",
    "# ov_nc = [nc for nc in doc_ov.noun_chunks]\n",
    "# ov_kt = [kt for kt in doc_ov._.combo_basic.sort_values(ascending=False).head(10).index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2aa6c5-c393-45d7-a097-0ee19f3c110d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def gen_tuples(a, b):\n",
    "#     return [(i,j.text) for j in b for i in a]\n",
    "\n",
    "# # for a,b in gen_tuples(ov_kt, ov_nc):\n",
    "# #     print(cosine_sim(a,b))\n",
    "\n",
    "# ov_tups = gen_tuples(ov_kt, ov_nc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f270b2f6-cff8-470d-b218-b830c3fefc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc_ov = nlp(ovdoc1)\n",
    "# print(doc_ov._.combo_basic.sort_values(ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63549242-7226-428a-ae4a-6abd72b22dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [ent.text for ent in doc_ov.ents if ent.label_ == \"ORG\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a75b3e1-982c-412f-a39b-4722fd9ad649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for chunk in doc_ov.noun_chunks:\n",
    "    print(f\"{chunk.text} | {chunk.root.text} | {chunk.label_} |\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1516841-511f-41e5-a955-5e8fb8df29f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ov_nc = [nc for nc in doc_ov.noun_chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdf6fe9-94ff-49fe-875a-b502b37ef394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc_incogdoc = nlp(incogdoc2)\n",
    "# print(doc_incogdoc._.combo_basic.sort_values(ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8d167d-197f-42a9-be43-e7e093293415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [ent.text for ent in doc_incogdoc.ents if ent.label_ == \"ORG\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea5fdf4-c78d-466e-9d36-7089e6426ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp = spacy.load(\"en_core_web_lg\")  # make sure to use larger package!\n",
    "# doc1 = nlp(ovdoc1)\n",
    "# doc2 = nlp(incogdoc2)\n",
    "# doc1.similarity(doc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15462a05-f614-41e0-8863-e0b464b92b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp_latin = spacy.load(\"/srv/fasttext/en_vectors\")\n",
    "# doc1 = nlp(ovdoc1)\n",
    "# doc2 = nlp(incogdoc2)\n",
    "# doc1.similarity(doc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b271bc28-acbd-4e11-9b0c-570ef7672fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set_gpu_allocator(\"pytorch\")\n",
    "# require_gpu(0)\n",
    "\n",
    "# nlp = spacy.load(\"en_core_web_trf\")\n",
    "# for doc in nlp.pipe([ovdoc1, incogdoc2]):\n",
    "#     tokvecs = doc._.trf_data.tensors[-1]\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c223f8d-6b9e-43e6-b553-ef4476dc0182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set_gpu_allocator(\"pytorch\")\n",
    "# require_gpu(0)\n",
    "\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f2de7c-d5f4-4fb4-997b-d1924b17cc1d",
   "metadata": {},
   "source": [
    "## BERT For Measuring Text Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb47c1d0-c57a-4282-adb8-d37f4aa4c963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import GPUtil\n",
    "import torch # the main pytorch library\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as f # the sub-library containing different functions for manipulating with tensors\n",
    "from thinc.api import set_gpu_allocator, require_gpu\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import re\n",
    "import tika\n",
    "from tika import parser\n",
    "from glob import glob\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, RegexpTokenizer\n",
    "\n",
    "set_gpu_allocator(\"pytorch\")\n",
    "print(require_gpu(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a9cc8aa-0b00-4810-afe8-70d76ae7c9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sents(pdf):\n",
    "    raw = parser.from_file(pdf)\n",
    "    content = raw['content'].strip()\n",
    "    datepat = r\"[0-9]{0,2}/[0-9]{0,2}/[0-9]{0,2}, [0-9]{0,2}:[0-9]{0,2} [AP]M\"\n",
    "    sents = [s.strip() for s in sent_tokenize(content)]\n",
    "    sents = [re.sub(datepat, '', s) for s in sents]\n",
    "    pattern = r\"Page [0-9]{0,3} of [0-9]{0,3}\"\n",
    "    urlpat = r'^https?:\\/\\/.*[\\r\\n]*'\n",
    "    sents = [re.sub(pattern, '', s) for s in sents]\n",
    "    sents = [re.sub(urlpat, '', s, flags=re.MULTILINE) for s in sents]\n",
    "    sents = [re.sub(r'\\n', ' ', s) for s in sents]\n",
    "    sents = [s for s in sents if s != '']\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    sents = [\" \".join(tokenizer.tokenize(s.lower())) for s in sents]\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0723c5ec-b6c6-4ee7-bb39-66f2d27877e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../mr_minio/data/openvault\"\n",
    "ovpdfs= glob(path+'/*Jersey*Open*pdf')\n",
    "\n",
    "path = \"../mr_minio/data/openvault\"\n",
    "fedpdfs = glob(path+'/*Feder*pdf')\n",
    "\n",
    "path = \"../mr_minio/data/openvault\"\n",
    "incogpdfs = glob(path+'/*Vancouver*pdf')\n",
    "\n",
    "path = \"../mr_minio/data/openvault\"\n",
    "intrapdfs = glob(path+'/*Buenos*pdf')\n",
    "\n",
    "ovdocs = [extract_sents(doc) for doc in ovpdfs]\n",
    "feddocs = [extract_sents(doc) for doc in fedpdfs]\n",
    "incogdocs = [extract_sents(doc) for doc in incogpdfs]\n",
    "intradocs = [extract_sents(doc) for doc in intrapdfs]\n",
    "\n",
    "ovsents = [sent for doc in ovdocs for sent in doc]\n",
    "fedsents = [sent for doc in feddocs for sent in doc]\n",
    "incogsents = [sent for doc in incogpdfs for sent in doc]\n",
    "intragsents = [sent for doc in intradocs for sent in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b71ba82d-7120-4388-8747-56893724067c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"Three years later, the coffin was still full of Jello.\",\n",
    "    \"The fish dreamed of escaping the fishbowl and into the toilet where he saw his friend go.\",\n",
    "    \"The person box was packed with jelly many dozens of months later.\",\n",
    "    \"He found a leprechaun in his walnut shell.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d0609d3-094b-4eae-9655-a301bbc2ef34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 768)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "\n",
    "sentence_embeddings = model.encode(sentences)\n",
    "sentence_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57ee149b-2452-4b6c-8359-242d4d70a8c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.33088905, 0.7219258 , 0.5548363 ]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(\n",
    "    [sentence_embeddings[0]],\n",
    "    sentence_embeddings[1:]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "441c3528-d99f-4b5f-8d73-600eebcccce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/bert-base-nli-mean-tokens')\n",
    "model = AutoModel.from_pretrained('sentence-transformers/bert-base-nli-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a960b042-a475-4672-aab7-b66f2f17d07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"Three years later, the coffin was still full of Jello.\",\n",
    "    \"The fish dreamed of escaping the fishbowl and into the toilet where he saw his friend go.\",\n",
    "    \"The person box was packed with jelly many dozens of months later.\",\n",
    "    \"He found a leprechaun in his walnut shell.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d60807d9-a26e-40ee-9e56-ef88a830413a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize dictionary to store tokenized sentences\n",
    "tokens = {'input_ids': [], 'attention_mask': []}\n",
    "\n",
    "for sentence in sentences:\n",
    "    # encode each sentence and append to dictionary\n",
    "    new_tokens = tokenizer.encode_plus(sentence, max_length=128,\n",
    "                                       truncation=True, padding='max_length',\n",
    "                                       return_tensors='pt')\n",
    "    tokens['input_ids'].append(new_tokens['input_ids'][0])\n",
    "    tokens['attention_mask'].append(new_tokens['attention_mask'][0])\n",
    "\n",
    "# reformat list of tensors into single tensor\n",
    "tokens['input_ids'] = torch.stack(tokens['input_ids'])\n",
    "tokens['attention_mask'] = torch.stack(tokens['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3ebb060-8162-4f65-8a5d-48ed8c9b0e90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['last_hidden_state', 'pooler_output'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model(**tokens)\n",
    "outputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9656a4a2-67e4-445e-8685-948229e7d9cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0692,  0.6230,  0.0354,  ...,  0.8033,  1.6314,  0.3281],\n",
       "         [ 0.0367,  0.6842,  0.1946,  ...,  0.0848,  1.4747, -0.3008],\n",
       "         [-0.0121,  0.6543, -0.0727,  ..., -0.0326,  1.7717, -0.6812],\n",
       "         ...,\n",
       "         [ 0.1953,  1.1085,  0.3390,  ...,  1.2826,  1.0114, -0.0728],\n",
       "         [ 0.0902,  1.0288,  0.3297,  ...,  1.2940,  0.9865, -0.1113],\n",
       "         [ 0.1240,  0.9737,  0.3933,  ...,  1.1359,  0.8768, -0.1043]],\n",
       "\n",
       "        [[-0.3212,  0.8251,  1.0554,  ..., -0.1855,  0.1517,  0.3937],\n",
       "         [-0.7146,  1.0297,  1.1217,  ...,  0.0331,  0.2382, -0.1563],\n",
       "         [-0.2352,  1.1353,  0.8594,  ..., -0.4310, -0.0272, -0.2968],\n",
       "         ...,\n",
       "         [-0.5400,  0.3236,  0.7839,  ...,  0.0022, -0.2994,  0.2659],\n",
       "         [-0.5643,  0.3187,  0.9576,  ...,  0.0342, -0.3030,  0.1878],\n",
       "         [-0.5172,  0.3599,  0.9336,  ...,  0.0243, -0.2232,  0.1672]],\n",
       "\n",
       "        [[-0.7576,  0.8399, -0.3792,  ...,  0.1271,  1.2514,  0.1365],\n",
       "         [-0.6591,  0.7614, -0.4662,  ...,  0.2259,  1.1289, -0.3611],\n",
       "         [-0.9007,  0.6791, -0.3778,  ...,  0.1142,  0.9080, -0.1830],\n",
       "         ...,\n",
       "         [-0.2158,  0.5463,  0.3117,  ...,  0.1802,  0.7169, -0.0672],\n",
       "         [-0.3092,  0.4833,  0.3021,  ...,  0.2289,  0.6656, -0.0932],\n",
       "         [-0.2940,  0.4678,  0.3095,  ...,  0.2782,  0.5144, -0.1021]],\n",
       "\n",
       "        [[-0.2362,  0.8551, -0.8040,  ...,  0.6122,  0.3003, -0.1492],\n",
       "         [-0.0868,  0.9531, -0.6419,  ...,  0.7867,  0.2960, -0.7350],\n",
       "         [-0.3016,  1.0148, -0.3380,  ...,  0.8634,  0.0463, -0.3623],\n",
       "         ...,\n",
       "         [-0.1090,  0.6320, -0.8433,  ...,  0.7485,  0.1025,  0.0149],\n",
       "         [ 0.0072,  0.7347, -0.7689,  ...,  0.6064,  0.1287,  0.0331],\n",
       "         [-0.1108,  0.7605, -0.4447,  ...,  0.6719,  0.1059, -0.0034]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = outputs.last_hidden_state\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "76fb989d-4635-4387-b86a-923450ffd2b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 128, 768])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85c4be3e-1836-4d8f-9813-2627484d719b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 128])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask = tokens['attention_mask']\n",
    "attention_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cfa70959-34d0-4450-9b35-ddc05c90b0ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 128, 768])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = attention_mask.unsqueeze(-1).expand(embeddings.size()).float()\n",
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d6f46226-1363-414c-86f4-9ccdf5c1227a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "80907c9b-8094-4552-816f-6226d6251960",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 128, 768])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_embeddings = embeddings * mask\n",
    "masked_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "39753275-afbd-476f-907c-eb1771730cd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0692,  0.6230,  0.0354,  ...,  0.8033,  1.6314,  0.3281],\n",
       "         [ 0.0367,  0.6842,  0.1946,  ...,  0.0848,  1.4747, -0.3008],\n",
       "         [-0.0121,  0.6543, -0.0727,  ..., -0.0326,  1.7717, -0.6812],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000, -0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000, -0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000, -0.0000]],\n",
       "\n",
       "        [[-0.3212,  0.8251,  1.0554,  ..., -0.1855,  0.1517,  0.3937],\n",
       "         [-0.7146,  1.0297,  1.1217,  ...,  0.0331,  0.2382, -0.1563],\n",
       "         [-0.2352,  1.1353,  0.8594,  ..., -0.4310, -0.0272, -0.2968],\n",
       "         ...,\n",
       "         [-0.0000,  0.0000,  0.0000,  ...,  0.0000, -0.0000,  0.0000],\n",
       "         [-0.0000,  0.0000,  0.0000,  ...,  0.0000, -0.0000,  0.0000],\n",
       "         [-0.0000,  0.0000,  0.0000,  ...,  0.0000, -0.0000,  0.0000]],\n",
       "\n",
       "        [[-0.7576,  0.8399, -0.3792,  ...,  0.1271,  1.2514,  0.1365],\n",
       "         [-0.6591,  0.7614, -0.4662,  ...,  0.2259,  1.1289, -0.3611],\n",
       "         [-0.9007,  0.6791, -0.3778,  ...,  0.1142,  0.9080, -0.1830],\n",
       "         ...,\n",
       "         [-0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000, -0.0000],\n",
       "         [-0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000, -0.0000],\n",
       "         [-0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000, -0.0000]],\n",
       "\n",
       "        [[-0.2362,  0.8551, -0.8040,  ...,  0.6122,  0.3003, -0.1492],\n",
       "         [-0.0868,  0.9531, -0.6419,  ...,  0.7867,  0.2960, -0.7350],\n",
       "         [-0.3016,  1.0148, -0.3380,  ...,  0.8634,  0.0463, -0.3623],\n",
       "         ...,\n",
       "         [-0.0000,  0.0000, -0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000, -0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [-0.0000,  0.0000, -0.0000,  ...,  0.0000,  0.0000, -0.0000]]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4e22a6ca-75bb-401e-9cc9-c71edf22da27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 768])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summed = torch.sum(masked_embeddings, 1)\n",
    "summed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0b038ade-c283-4e0c-9817-ec98e4a9997f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 768])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summed_mask = torch.clamp(mask.sum(1), min=1e-9)\n",
    "summed_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f83c9600-7c0a-49ba-9061-4b267d44869f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[15., 15., 15.,  ..., 15., 15., 15.],\n",
       "        [22., 22., 22.,  ..., 22., 22., 22.],\n",
       "        [15., 15., 15.,  ..., 15., 15., 15.],\n",
       "        [14., 14., 14.,  ..., 14., 14., 14.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summed_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2511ec96-c160-497f-9b42-48741ccdc769",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0745,  0.8637,  0.1795,  ...,  0.7734,  1.7247, -0.1803],\n",
       "        [-0.3715,  0.9729,  1.0840,  ..., -0.2552, -0.2759,  0.0358],\n",
       "        [-0.5030,  0.7950, -0.1240,  ...,  0.1441,  0.9704, -0.1791],\n",
       "        [-0.2131,  1.0175, -0.8833,  ...,  0.7371,  0.1947, -0.3011]],\n",
       "       grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_pooled = summed / summed_mask\n",
    "mean_pooled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c63faf4f-b0f6-4bb3-8477-451ec1ed8457",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.33088908, 0.7219258 , 0.5548364 ]], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert from PyTorch tensor to numpy array\n",
    "mean_pooled = mean_pooled.detach().cpu().numpy()\n",
    "\n",
    "# calculate\n",
    "cosine_similarity(\n",
    "    [mean_pooled[0]],\n",
    "    mean_pooled[1:]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820ae858-a2c8-42f8-9570-1256928b104d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb98b5b-392c-4718-b393-3b2fbafd873d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b48eac-809a-47b9-8153-b7802569cd96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8d609d-cb10-4c30-b086-7d5ead521196",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
